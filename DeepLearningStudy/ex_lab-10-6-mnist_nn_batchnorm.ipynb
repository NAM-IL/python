{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">[</span> ex_lab-10-6-mnist_nn_batchnorm <span style=\"color:blue\">]</span>\n",
    "<p>출처: <a href=\"http://hunkim.github.io/ml/\" title=\"모두를 위한 머신러닝과 딥러닝의 강의\" target=\"blank\">모두를 위한 머신러닝과 딥러닝의 강의</a></p> <br/>\n",
    "> [Tensorflow Document(Tensor Transformations)](https://www.tensorflow.org/api_guides/python/array_ops) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Batchnormalization Layer\n",
    "\n",
    "## What is a batchnormalization layer?\n",
    "It is a layer that normalize the output before the activation layer. [The original paper](https://arxiv.org/abs/1502.03167) was proposed by Sergey Ioffe in 2015.\n",
    "\n",
    "Batch Normalization Layer looks like this: ![bn](https://kratzert.github.io/images/bn_backpass/bn_algorithm.PNG)\n",
    "\n",
    "## Why batchnormalization?\n",
    "The distribution of each layer's input changes because the weights of the previous layer change as we update weights by the gradient descent. This is called a covariance shift, which makes the network training difficult.\n",
    "\n",
    "For example, if the activation layer is a relu layer and the input of the activation layer is shifted to less than zeros, no weights will be activated!\n",
    "\n",
    "One thing also worth mentioning is that $\\gamma$ and $\\beta$ parameters in $$ y = \\gamma \\hat{x} + \\beta $$ are also trainable. \n",
    "\n",
    "**What it means is that if we don't need the batchnormalization, its parameters will be updated such that it offsets the normalization step.**\n",
    "\n",
    "For example, assume that\n",
    "\n",
    "\\begin{align}\n",
    "\\gamma &= \\sqrt{\\sigma^2_B + \\epsilon}\\\\\n",
    "\\beta &= \\mu_B\n",
    "\\end{align}\n",
    "\n",
    "then\n",
    "\n",
    "$$ y_i = \\gamma \\hat{x_i} + \\beta = x_i $$\n",
    "\n",
    "Also note that $\\mu$ and $\\sigma$ are computed using moving averages during the training step. However, during the test time, the computed $\\mu$ and $\\sigma$ will be used as fixed\n",
    "\n",
    "## Conclusion\n",
    "- Always use the batch normalization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enough Talk: how to implement in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Library\n",
    "- We use the famous MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Model & Solver Class\n",
    "- Object-Oriented-Programming allows to define multiple model easily\n",
    "- Why do we separate model and solver classes?\n",
    "  - We can just swap out the model class in the Solver class when we need a different network architecture\n",
    "  - Usually we need one solver class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Network Model Class\n",
    "    \n",
    "    Note that this class has only the constructor.\n",
    "    The actual model is defined inside the constructor.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    X : tf.float32\n",
    "        This is a tensorflow placeholder for MNIST images\n",
    "        Expected shape is [None, 784]\n",
    "        \n",
    "    y : tf.float32\n",
    "        This is a tensorflow placeholder for MNIST labels (one hot encoded)\n",
    "        Expected shape is [None, 10]\n",
    "        \n",
    "    mode : tf.bool\n",
    "        This is used for the batch normalization\n",
    "        It's `True` at training time and `False` at test time\n",
    "        \n",
    "    loss : tf.float32\n",
    "        The loss function is a softmax cross entropy\n",
    "        \n",
    "    train_op\n",
    "        This is simply the training op that minimizes the loss\n",
    "        \n",
    "    accuracy : tf.float32\n",
    "        The accuracy operation\n",
    "        \n",
    "    \n",
    "    Examples\n",
    "    ----------\n",
    "    >>> model = Model(\"Batch Norm\", 32, 10)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, name, input_dim, output_dim, hidden_dims=[32, 32], use_batchnorm=True, activation_fn=tf.nn.relu, optimizer=tf.train.AdamOptimizer, lr=0.01):\n",
    "        \"\"\" Constructor\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        name : str\n",
    "            The name of this network\n",
    "            The entire network will be created under `tf.variable_scope(name)`\n",
    "            \n",
    "        input_dim : int\n",
    "            The input dimension\n",
    "            In this example, 784\n",
    "        \n",
    "        output_dim : int\n",
    "            The number of output labels\n",
    "            There are 10 labels\n",
    "            \n",
    "        hidden_dims : list (default: [32, 32])\n",
    "            len(hidden_dims) = number of layers\n",
    "            each element is the number of hidden units\n",
    "            \n",
    "        use_batchnorm : bool (default: True)\n",
    "            If true, it will create the batchnormalization layer\n",
    "            \n",
    "        activation_fn : TF functions (default: tf.nn.relu)\n",
    "            Activation Function\n",
    "            \n",
    "        optimizer : TF optimizer (default: tf.train.AdamOptimizer)\n",
    "            Optimizer Function\n",
    "            \n",
    "        lr : float (default: 0.01)\n",
    "            Learning rate\n",
    "        \n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            # Placeholders are defined\n",
    "            self.X = tf.placeholder(tf.float32, [None, input_dim], name='X')\n",
    "            self.y = tf.placeholder(tf.float32, [None, output_dim], name='y')\n",
    "            self.mode = tf.placeholder(tf.bool, name='train_mode')            \n",
    "            \n",
    "            # Loop over hidden layers\n",
    "            net = self.X\n",
    "            for i, h_dim in enumerate(hidden_dims):\n",
    "                with tf.variable_scope('layer{}'.format(i)):\n",
    "                    net = tf.layers.dense(net, h_dim)\n",
    "                    \n",
    "                    if use_batchnorm:\n",
    "                        net = tf.layers.batch_normalization(net, training=self.mode)\n",
    "                        \n",
    "                    net = activation_fn(net)\n",
    "            \n",
    "            # Attach fully connected layers\n",
    "            net = tf.contrib.layers.flatten(net)\n",
    "            net = tf.layers.dense(net, output_dim)\n",
    "            \n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=self.y)\n",
    "            self.loss = tf.reduce_mean(self.loss, name='loss')    \n",
    "            \n",
    "            # When using the batchnormalization layers,\n",
    "            # it is necessary to manually add the update operations\n",
    "            # because the moving averages are not included in the graph            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)\n",
    "            with tf.control_dependencies(update_ops):                     \n",
    "                self.train_op = optimizer(lr).minimize(self.loss)\n",
    "            \n",
    "            # Accuracy etc \n",
    "            softmax = tf.nn.softmax(net, name='softmax')\n",
    "            self.accuracy = tf.equal(tf.argmax(softmax, 1), tf.argmax(self.y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.accuracy, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    \"\"\"Solver class\n",
    "    \n",
    "    This class will contain the model class and session\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    model : Model class\n",
    "    sess : TF session\n",
    "        \n",
    "    Methods\n",
    "    ----------\n",
    "    train(X, y)\n",
    "        Run the train_op and Returns the loss\n",
    "        \n",
    "    evalulate(X, y, batch_size=None)\n",
    "        Returns \"Loss\" and \"Accuracy\"\n",
    "        If batch_size is given, it's computed using batch_size\n",
    "        because most GPU memories cannot handle the entire training data at once\n",
    "            \n",
    "    Example\n",
    "    ----------\n",
    "    >>> sess = tf.InteractiveSession()\n",
    "    >>> model = Model(\"BatchNorm\", 32, 10)\n",
    "    >>> solver = Solver(sess, model)\n",
    "    \n",
    "    # Train\n",
    "    >>> solver.train(X, y)\n",
    "    \n",
    "    # Evaluate\n",
    "    >>> solver.evaluate(X, y)\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, model):\n",
    "        self.model = model\n",
    "        self.sess = sess\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        feed = {\n",
    "            self.model.X: X,\n",
    "            self.model.y: y,\n",
    "            self.model.mode: True\n",
    "        }\n",
    "        train_op = self.model.train_op\n",
    "        loss = self.model.loss\n",
    "        \n",
    "        return self.sess.run([train_op, loss], feed_dict=feed)\n",
    "    \n",
    "    def evaluate(self, X, y, batch_size=None):\n",
    "        if batch_size:\n",
    "            N = X.shape[0]\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            \n",
    "            for i in range(0, N, batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                \n",
    "                feed = {\n",
    "                    self.model.X: X_batch,\n",
    "                    self.model.y: y_batch,\n",
    "                    self.model.mode: False\n",
    "                }\n",
    "                \n",
    "                loss = self.model.loss\n",
    "                accuracy = self.model.accuracy\n",
    "                \n",
    "                step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed)\n",
    "                \n",
    "                total_loss += step_loss * X_batch.shape[0]\n",
    "                total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "            total_loss /= N\n",
    "            total_acc /= N\n",
    "            \n",
    "            return total_loss, total_acc\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            feed = {\n",
    "                self.model.X: X,\n",
    "                self.model.y: y,\n",
    "                self.model.mode: False\n",
    "            }\n",
    "            \n",
    "            loss = self.model.loss            \n",
    "            accuracy = self.model.accuracy\n",
    "\n",
    "            return self.sess.run([loss, accuracy], feed_dict=feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiate Model/Solver classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "output_dim = 10\n",
    "N = 55000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# We create two models: one with the batch norm and other without\n",
    "bn = Model('batchnorm', input_dim, output_dim, use_batchnorm=True)\n",
    "nn = Model('no_norm', input_dim, output_dim, use_batchnorm=False)\n",
    "\n",
    "# We create two solvers: to train both models at the same time for comparison\n",
    "# Usually we only need one solver class\n",
    "bn_solver = Solver(sess, bn)\n",
    "nn_solver = Solver(sess, nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_n = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Save Losses and Accuracies every epoch\n",
    "# We are going to plot them later\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run the train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0-TRAIN] Batchnorm Loss(Acc): 0.12367(96.17%) vs No Batchnorm Loss(Acc): 0.20167(93.94%)\n",
      "[Epoch 0-VALID] Batchnorm Loss(Acc): 0.14051(95.94%) vs No Batchnorm Loss(Acc): 0.19681(94.02%)\n",
      "[Epoch 1-TRAIN] Batchnorm Loss(Acc): 0.09652(97.01%) vs No Batchnorm Loss(Acc): 0.18145(94.67%)\n",
      "[Epoch 1-VALID] Batchnorm Loss(Acc): 0.10934(96.88%) vs No Batchnorm Loss(Acc): 0.19198(94.64%)\n",
      "[Epoch 2-TRAIN] Batchnorm Loss(Acc): 0.09468(97.04%) vs No Batchnorm Loss(Acc): 0.18951(94.73%)\n",
      "[Epoch 2-VALID] Batchnorm Loss(Acc): 0.11142(96.70%) vs No Batchnorm Loss(Acc): 0.20398(94.74%)\n",
      "[Epoch 3-TRAIN] Batchnorm Loss(Acc): 0.07783(97.52%) vs No Batchnorm Loss(Acc): 0.15787(95.52%)\n",
      "[Epoch 3-VALID] Batchnorm Loss(Acc): 0.10097(97.06%) vs No Batchnorm Loss(Acc): 0.19575(95.12%)\n",
      "[Epoch 4-TRAIN] Batchnorm Loss(Acc): 0.07098(97.85%) vs No Batchnorm Loss(Acc): 0.14513(95.77%)\n",
      "[Epoch 4-VALID] Batchnorm Loss(Acc): 0.10442(97.08%) vs No Batchnorm Loss(Acc): 0.19022(95.36%)\n",
      "[Epoch 5-TRAIN] Batchnorm Loss(Acc): 0.07349(97.71%) vs No Batchnorm Loss(Acc): 0.15477(95.62%)\n",
      "[Epoch 5-VALID] Batchnorm Loss(Acc): 0.10412(96.86%) vs No Batchnorm Loss(Acc): 0.19285(94.90%)\n",
      "[Epoch 6-TRAIN] Batchnorm Loss(Acc): 0.05821(98.11%) vs No Batchnorm Loss(Acc): 0.14781(95.84%)\n",
      "[Epoch 6-VALID] Batchnorm Loss(Acc): 0.09621(97.20%) vs No Batchnorm Loss(Acc): 0.19791(94.84%)\n",
      "[Epoch 7-TRAIN] Batchnorm Loss(Acc): 0.05276(98.38%) vs No Batchnorm Loss(Acc): 0.12125(96.58%)\n",
      "[Epoch 7-VALID] Batchnorm Loss(Acc): 0.09031(97.22%) vs No Batchnorm Loss(Acc): 0.19596(95.46%)\n",
      "[Epoch 8-TRAIN] Batchnorm Loss(Acc): 0.04735(98.53%) vs No Batchnorm Loss(Acc): 0.11564(96.73%)\n",
      "[Epoch 8-VALID] Batchnorm Loss(Acc): 0.09221(97.28%) vs No Batchnorm Loss(Acc): 0.17220(95.70%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c3f4c646e639>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mb_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mn_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-1ba96dc6b816>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, X, y, batch_size)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0mstep_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstep_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\django\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\django\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\django\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\django\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\django\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    for _ in range(N//batch_size):\n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _, bn_loss = bn_solver.train(X_batch, y_batch)\n",
    "        _, nn_loss = nn_solver.train(X_batch, y_batch)       \n",
    "    \n",
    "    b_loss, b_acc = bn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
    "    n_loss, n_acc = nn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
    "    \n",
    "    # Save train losses/acc\n",
    "    train_losses.append([b_loss, n_loss])\n",
    "    train_accs.append([b_acc, n_acc])\n",
    "    print('[Epoch {0}-TRAIN] Batchnorm Loss(Acc): {1:.5f}({2:.2%}) vs No Batchnorm Loss(Acc): {3:.5f}({4:.2%})'.format(\n",
    "                epoch, b_loss, b_acc, n_loss, n_acc))\n",
    "    \n",
    "    b_loss, b_acc = bn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
    "    n_loss, n_acc = nn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
    "    \n",
    "    # Save valid losses/acc\n",
    "    valid_losses.append([b_loss, n_loss])\n",
    "    valid_accs.append([b_acc, n_acc])\n",
    "    print('[Epoch {0}-VALID] Batchnorm Loss(Acc): {1:.5f}({2:.2%}) vs No Batchnorm Loss(Acc): {3:.5f}({4:.2%})'.format(\n",
    "                epoch, b_loss, b_acc, n_loss, n_acc))\n",
    "    \n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Performance Comparison\n",
    "* With the batchnormalization, the loss is lower and it's more accurate too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_solver.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_solver.evaluate(mnist.test.images, mnist.test.labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_compare(loss_list: list, ylim=None, title=None) -> None:\n",
    "    \n",
    "    bn = [i[0] for i in loss_list]\n",
    "    nn = [i[1] for i in loss_list]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(bn, label='With BN')\n",
    "    plt.plot(nn, label='Without BN')\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "        \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_compare(train_losses, title='Training Loss at Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_compare(train_accs, [0, 1.0], title=\"Training Acc at Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_compare(valid_losses, title='Validation Loss at Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_compare(valid_accs, [0, 1.], title='Validation Acc at Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
