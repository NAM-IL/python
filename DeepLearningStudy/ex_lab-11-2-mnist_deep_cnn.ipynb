{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">[</span> Lab 11-2 : MNIST and Deep learning CNN <span style=\"color:blue\">]</span>\n",
    " <p>출처: <a href=\"http://hunkim.github.io/ml/\" title=\"모두를 위한 머신러닝과 딥러닝의 강의\" target=\"blank\">모두를 위한 머신러닝과 딥러닝의 강의</a></p> <br/>\n",
    " > [CS 20SI: Tensorflow for Deep Learning Research](http://web.stanford.edu/class/cs20si/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lab 11 MNIST and Deep learning CNN\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read in data\n",
    "> using TF Learn's built in function to load MNIST data to the folder data/mnist <br/>\n",
    "> Check out [MNIST For ML Beginners](https://www.tensorflow.org/get_started/mnist/beginners) for more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define paramaters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: create placeholders for features and labels\n",
    "> each image in the MNIST data is of shape 28*28 = 784 <br/>\n",
    "> therefore, each image is represented with a 1x784 tensor <br/>\n",
    "> there are 10 classes for each image, corresponding to digits 0 - 9.  <br/>\n",
    "> each lable is one hot vector. <br/>\n",
    "> We'll be doing dropout for hidden layer so we'll need a placeholder for the dropout probability too <br/>\n",
    "> Use None for shape so we can change the batch_size once we've built the graph <br/>\n",
    ">> dropout (keep_prob) rate  0.5 ~ 0.7 on training, but should be 1 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 + 5: create weights + do inference\n",
    "> the model is <br/>\n",
    "conv $\\rightarrow$ relu $\\rightarrow$ pool $\\rightarrow$ dropOut  $\\rightarrow$ <br/>\n",
    "conv $\\rightarrow$ relu $\\rightarrow$ pool $\\rightarrow$ dropOut  $\\rightarrow$ <br/>\n",
    "conv $\\rightarrow$ relu $\\rightarrow$ pool $\\rightarrow$ dropOut $\\rightarrow$  <br/>\n",
    "relu $\\rightarrow$ dropOut  $\\rightarrow$  <br/>\n",
    "fully connected $\\rightarrow$ softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"add_1:0\", shape=(?, 10), dtype=float32)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "'''\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# L3 ImgIn shape=(?, 7, 7, 64)\n",
    "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "#    Conv      ->(?, 7, 7, 128)\n",
    "#    Pool      ->(?, 4, 4, 128)\n",
    "#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n",
    "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
    "\n",
    "'''\n",
    "Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n",
    "'''\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# L4 FC 4x4x128 inputs -> 625 outputs\n",
    "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n",
    "Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n",
    "'''\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# L5 Final FC 625 inputs -> 10 outputs\n",
    "W5 = tf.get_variable(\"W5\", shape=[625, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logits = tf.matmul(L4, W5) + b5\n",
    "\n",
    "\n",
    "'''\n",
    "Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: define loss function\n",
    "> use softmax cross entropy with logits as the loss function\n",
    "> compute mean cross entropy, softmax is applied internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: define training op\n",
    "> using AdamOptimizer with learning rate of {learning_rate} to minimize cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize session & global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.420728283\n",
      "Epoch: 0002 cost = 0.093831111\n",
      "Epoch: 0003 cost = 0.068687887\n",
      "Epoch: 0004 cost = 0.057447360\n",
      "Epoch: 0005 cost = 0.048385200\n",
      "Epoch: 0006 cost = 0.043854826\n",
      "Epoch: 0007 cost = 0.038436120\n",
      "Epoch: 0008 cost = 0.038062397\n",
      "Epoch: 0009 cost = 0.034725077\n",
      "Epoch: 0010 cost = 0.031151260\n",
      "Epoch: 0011 cost = 0.031648903\n",
      "Epoch: 0012 cost = 0.029351540\n",
      "Epoch: 0013 cost = 0.027477079\n",
      "Epoch: 0014 cost = 0.028468259\n",
      "Epoch: 0015 cost = 0.024895316\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model and check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9932\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1qtIJnSiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e353e37d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nLearning stared. It takes sometime.\\nEpoch: 0001 cost = 0.385748474\\nEpoch: 0002 cost = 0.092017397\\nEpoch: 0003 cost = 0.065854684\\nEpoch: 0004 cost = 0.055604566\\nEpoch: 0005 cost = 0.045996377\\nEpoch: 0006 cost = 0.040913645\\nEpoch: 0007 cost = 0.036924479\\nEpoch: 0008 cost = 0.032808939\\nEpoch: 0009 cost = 0.031791007\\nEpoch: 0010 cost = 0.030224456\\nEpoch: 0011 cost = 0.026849916\\nEpoch: 0012 cost = 0.026826763\\nEpoch: 0013 cost = 0.027188021\\nEpoch: 0014 cost = 0.023604777\\nEpoch: 0015 cost = 0.024607201\\nLearning Finished!\\nAccuracy: 0.9938\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "\n",
    "# if you have a OOM error, please refer to lab-11-X-mnist_deep_cnn_low_memory.py\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={ X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "\n",
    "# show image\n",
    "plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Learning stared. It takes sometime.\n",
    "Epoch: 0001 cost = 0.385748474\n",
    "Epoch: 0002 cost = 0.092017397\n",
    "Epoch: 0003 cost = 0.065854684\n",
    "Epoch: 0004 cost = 0.055604566\n",
    "Epoch: 0005 cost = 0.045996377\n",
    "Epoch: 0006 cost = 0.040913645\n",
    "Epoch: 0007 cost = 0.036924479\n",
    "Epoch: 0008 cost = 0.032808939\n",
    "Epoch: 0009 cost = 0.031791007\n",
    "Epoch: 0010 cost = 0.030224456\n",
    "Epoch: 0011 cost = 0.026849916\n",
    "Epoch: 0012 cost = 0.026826763\n",
    "Epoch: 0013 cost = 0.027188021\n",
    "Epoch: 0014 cost = 0.023604777\n",
    "Epoch: 0015 cost = 0.024607201\n",
    "Learning Finished!\n",
    "Accuracy: 0.9938\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
