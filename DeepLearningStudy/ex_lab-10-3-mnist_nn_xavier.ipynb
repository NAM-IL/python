{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">[</span> lab-10-3-MNIST_NN_xavier <span style=\"color:blue\">]</span>\n",
    "<p>출처: <a href=\"http://hunkim.github.io/ml/\" title=\"모두를 위한 머신러닝과 딥러닝의 강의\" target=\"blank\">모두를 위한 머신러닝과 딥러닝의 강의</a></p> <br/>\n",
    "> [Tensorflow Document(Tensor Transformations)](https://www.tensorflow.org/api_guides/python/array_ops)  <br/>\n",
    "> [CS 20SI: Tensorflow for Deep Learning Research](http://web.stanford.edu/class/cs20si/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 MNIST and Xavier\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "print('tensorflow version: {0}'.format(tf.__version__))\n",
    "# print('numpy version: {0}'.format(np.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define paramaters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read in data\n",
    "> using TF Learn's built in function to load MNIST data to the folder data/mnist <br/>\n",
    "> Check out [MNIST For ML Beginners](https://www.tensorflow.org/get_started/mnist/beginners) for more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: create placeholders for features and labels\n",
    "> each image in the MNIST data is of shape 28*28 = 784 <br/>\n",
    "> therefore, each image is represented with a 1x784 tensor <br/>\n",
    "> there are 10 classes for each image, corresponding to digits 0 - 9.  <br/>\n",
    "> each lable is one hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: create weights and bias\n",
    "> weights & bias for nn layers ( [How to do Xavier initialization on TensorFlow\n",
    "](http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow) )<br/>\n",
    ">> w is initialized by using Xavier initializer.   <br/>\n",
    ">> b is initialized to random variables with normal random distribution. <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: build model\n",
    "> the model is passed through RELU to compute rectified linear, and then returns the logits. <br/>\n",
    "> this logits will be later passed through softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "hypothesis = tf.matmul(L2, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: define loss function\n",
    "> use cross entropy of softmax of logits as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: define training op\n",
    ">using Adam algorithm with learning rate of {learning_rate} to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize session & global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.297091865\n",
      "Epoch: 0002 cost = 0.114303470\n",
      "Epoch: 0003 cost = 0.073683188\n",
      "Epoch: 0004 cost = 0.051510853\n",
      "Epoch: 0005 cost = 0.038488539\n",
      "Epoch: 0006 cost = 0.031366156\n",
      "Epoch: 0007 cost = 0.024205126\n",
      "Epoch: 0008 cost = 0.019621657\n",
      "Epoch: 0009 cost = 0.015520355\n",
      "Epoch: 0010 cost = 0.013323756\n",
      "Epoch: 0011 cost = 0.010857077\n",
      "Epoch: 0012 cost = 0.012240492\n",
      "Epoch: 0013 cost = 0.010399937\n",
      "Epoch: 0014 cost = 0.010385135\n",
      "Epoch: 0015 cost = 0.009287596\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model and check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9775\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21784804e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nEpoch: 0001 cost = 0.301498963\\nEpoch: 0002 cost = 0.107252513\\nEpoch: 0003 cost = 0.064888892\\nEpoch: 0004 cost = 0.044463030\\nEpoch: 0005 cost = 0.029951642\\nEpoch: 0006 cost = 0.020663404\\nEpoch: 0007 cost = 0.015853033\\nEpoch: 0008 cost = 0.011764387\\nEpoch: 0009 cost = 0.008598264\\nEpoch: 0010 cost = 0.007383116\\nEpoch: 0011 cost = 0.006839140\\nEpoch: 0012 cost = 0.004672963\\nEpoch: 0013 cost = 0.003979437\\nEpoch: 0014 cost = 0.002714260\\nEpoch: 0015 cost = 0.004707661\\nLearning Finished!\\nAccuracy: 0.9783\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "Epoch: 0001 cost = 0.301498963\n",
    "Epoch: 0002 cost = 0.107252513\n",
    "Epoch: 0003 cost = 0.064888892\n",
    "Epoch: 0004 cost = 0.044463030\n",
    "Epoch: 0005 cost = 0.029951642\n",
    "Epoch: 0006 cost = 0.020663404\n",
    "Epoch: 0007 cost = 0.015853033\n",
    "Epoch: 0008 cost = 0.011764387\n",
    "Epoch: 0009 cost = 0.008598264\n",
    "Epoch: 0010 cost = 0.007383116\n",
    "Epoch: 0011 cost = 0.006839140\n",
    "Epoch: 0012 cost = 0.004672963\n",
    "Epoch: 0013 cost = 0.003979437\n",
    "Epoch: 0014 cost = 0.002714260\n",
    "Epoch: 0015 cost = 0.004707661\n",
    "Learning Finished!\n",
    "Accuracy: 0.9783\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
